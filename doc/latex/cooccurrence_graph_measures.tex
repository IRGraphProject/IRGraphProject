\documentclass[11pt, a4paper]{article}
\usepackage{ifxetex}
\usepackage{amsmath}
\usepackage[hyperindex,colorlinks,citecolor=cyan,linkcolor=gray,urlcolor=blue
			]{hyperref}

\ifxetex
	\usepackage{fontspec}
	\usepackage{unicode-math}
	\setmainfont[Ligatures=TeX,
		Extension=.otf,
		BoldFont=*-bold,
		UprightFont=*-regular,
		ItalicFont=*-italic,
		BoldItalicFont=*-bolditalic,
	SmallCapsFeatures={Letters=SmallCaps}]{texgyrepagella}
	\setmathfont[Ligatures=TeX]{texgyrepagella-math.otf}
	
	% set up Heros (Helvetica)
	\setsansfont[Ligatures=TeX,
		Extension=.otf,
		BoldFont=*-bold,
		UprightFont=*-regular,
		ItalicFont=*-italic,
		BoldItalicFont=*-bolditalic,
	SmallCapsFeatures={Letters=SmallCaps}]{texgyreheros}
\else
	\usepackage[utf8]{inputenc}
	\usepackage[T1]{fontenc}
\fi

\usepackage{ngerman}
\usepackage{booktabs}
\usepackage{microtype}

\usepackage{svg}
\usepackage{graphicx}
\graphicspath{{../img/}}

\title{Grapheigenschaften auf Kookkurrenzgraphen in Leichter und Standardsprache auf Wikipedia- und Nachrichtencorpora}
\author{Author 1, Author 2, Author 3\\Modul "`Fortgeschrittene Methoden des Information Retrieval"'}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation und Ziel}

Leichte oder auch Einfache Sprache ist eine Untermenge der deutschen Sprache,
die auf besonders leichte Verst\"andlichkeit optimiert ist. Sie umfasst unter
anderem spezielle Sprachregelungen, typographische Empfehlungen und
Rechtschreibregeln. 

Das \emph{Netzwerk Leichte Sprache} definiert folgende grundlegenden
Eigenschaften Leichter Sprache:

% TODO: Quellenangabe für das Folgende!
\begin{enumerate}
	\item Benutzen Sie einfache W\"orter
	\item Benutzen Sie W\"orter, die etwas genau beschreiben
	\item Benutzen Sie bekannte W\"orter und verzichten sie auf Fachw\"orter und Fremdw\"orter
	\item Benutzen Sie immer die gleichen W\"orter f\"ur gleiche Dinge
	\item Benutzen Sie kurze W\"orter
	\item Verzichten Sie auf Abk\"urzungen
	\item Benutzen Sie Verben
	\item Vermeiden Sie den Genitiv und Konjunktiv
	\item Vermeiden Sie Kolloquialismen und bildliche Sprache
	\item Benutzen Sie Ziffern anstatt von Worten
	\item Schreiben Sie kurze S\"atze, die nur eine Aussage enthalten
	\item Benutzen Sie einen einfachen Satzbau
	\item Schreiben Sie jeden Satz in eine Zeile
\end{enumerate}

Das englische \"Aquivalent zu Leichter Sprache ist \emph{Simple English}. Es
existieren verschiedene Modelle des Simple English, welche unterschiedliche
Ziele erreichen sollen -- z.B. das \emph{Simplified Technical English}, eine
kontrollierte Sprache f\"ur technische Handb\"ucher. Aufgrund dieser
konkurrierenden Ans\"atze gibt es keine einheitliche Definition oder
Sprachpraxis des Simple English. So sind z.B. i TODO ???


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tools}

\subsection{Programmiersprache Python}
Nach sprachunanbängigen Recherchen über verwendbare Bibliotheken haben wir uns wegen der bereits implementierten Graphalgorithmen in \texttt{graph-tool} für die Programmiersprache Python\footnote{\url{https://www.python.org/}} entschieden. Wegen der nativen Unicode-Unterstützung wurde Python 3 gewählt.

\subsection{Graphen-Bibliothek \texttt{graph-tool}}
\texttt{graph-tool}\footnote{\url{http://graph-tool.skewed.de/}} ist ein Python-Modul,
welches der Erstellung, Manipulation und statistischen
Auswertung von Graphen dient. Es stellt im Kern ein C++-Wrapper um die Boost
Graph Library dar, wodurch eine \"ahnliche Performanz zu nativen C-Bibliotheken
erreicht wird. Es ist zus\"atzlich in der Lage, Graphen mit modernen Techniken
zu visualisieren und \"ubliche Ma\ss{}e wie Clustering-Koeffizienten, Knoten-
und Kantengrade und Durchmesser zu berechnen.

\subsection{NLP-Framework NLTK}
Das Natural Language Toolkit (NLTK\footnote{\url{http://www.nltk.org/}}) ist ein Framework f\"ur Sprachverarbeitung in Python.
Die verwendeten Stopwortlisten für Deutsch und Englisch stammen aus der Bibliothek.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datenbasis}

Als Datenbasis wurden Quellen gew\"ahlt, die sich durch eine gro\ss{}e
semantische und logische N\"ahe auszeichnen, namentlich Nachrichtenseiten in
deutscher und Wikipediaartikel in englischer Sprache.

\subsection{Nachrichtenseiten}
\label{nrseiten}

\subsubsection{nachrichtenleicht.de (nl)}

Nachrichtenleicht ist ein Dienst des Deutschlandfunks, welcher einmal
w\"ochentlich die wichtigsten Nachrichten der vorangegangenen Woche in Leichter
Sprache zusammenfasst. Er soll die mehreren Millionen Menschen in Deutschland
erreichen, die aus verschiedenen Gr\"unden von konventionellen
Nachrichtenangeboten ausgeschlossen sind.

\subsubsection{deunews2010\_100K Corpus der ASV (denews10k)}

Als korrespondierende Datenquelle in Standardsprache wurde der
deunews\-2010\_100K-Corpus des Lehrstuhls f\"ur automatische Sprachverarbeitung
der Universit\"at Leipzig gew\"ahlt. Dieser weist eine \"ahnliche Satzzahl zu
den von nachrichtenleicht extrahierten Text aus und wurde bereits von der ASV
normalisiert.

\subsection{Wikipedia (wiki\_sim bzw. wiki\_en)}

Die zweite Datenquelle sind Artikel aus der Wikipedia in \emph{Simple} und
\emph{Standard English}. Mit fast 120.000 Lemmata stellt die Simple English
Wikipedia eine der gr\"o\ss{}ten \"offentlich frei verf\"ugbaren Textsammlungen
in einer einfachen Sprachvariante dar. Zus\"atzlich ist sie die
einzige, welche Dokumentenalignment aufweist, was die einfache Erstellung zwei
inhaltlich homogener Corpora gestattet.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Workflow}

\subsection{Datenextraktion und Erstellung der Corpora}
\label{datextr}

Mittels selbstgeschriebener Crawler wurden die relevanten Textmengen
heruntergeladen, extrahiert und zu Corpora zusammengefasst.

Nachrichtenleicht wurde rekursiv von der Startseite aus gequeriet
und die resultierenden Texte in einer MongoDB gespeichert.
Dabei wurden Überschriften, Teaser und Nachrichtentext jeweils seperat erfasst
und schließlich Teaser und Nachrichteninhalt zum Corpus hinzugefügt.

Zur Erstellung der Wikipedia-Corpora wurde zunächst eine Liste aller
in Simple English verfügbaren Artikel erstellt. Anschließend wurden diese und
die jeweils korrespondierenden Artikel in Standard English heruntergeladen und
im Rohformat (mit eingebettetem MediaWiki-Markup) in einer JSON-Datei gespeichert.
\\
Die Entfernung des Markups stellte sich als eine nichttriviale Aufgabe dar.
Der vollständige MWParser\footnote{MediaWiki
Parser from Hell (\url{http://mwparserfromhell.rtfd.org})} erwies sich für die
entsprechende Datenmenge (ca. 2~GB Rohdaten) als deutlich zu langsam
und somit unbrauchbar.
\\
Mit einem Python-Skript, das Markup mit Hilfe regul\"arer Ausdr\"ucke entfernt
bzw. umwandelt, konnten jedoch zufriedenstellende Ergebnisse mit vertretbarem
Zeitaufwand erzielt werden.
Nach Entfernen des Wiki-Markups wurden die Artikel auf die Satzanzahl
des jweils k\"urzeren Artikels reduziert und anschliessend in einer MongoDB
gespeichert.


\subsection{Berechnung der Kookkurrenzen}

Die Kookkurrenzen wurden von mittels der Pipeline des Lehrsstuhls f\"ur
Automatische Sprachverarbeitung für uns berechnet und in eine MySQL-Datenbank
gespeichert.


\subsection{Erstellung der Graphen}

Die errechneten Kookkurrenzen wurden in \texttt{graph-tool}-Objekte geparst.
Subgraphen
aus den Corpora wurden mittels eines \texttt{force-directed graph} nach
\cite{Hu2006} gelayoutet.
Die relevanten Daten zur Erstellung der Histogramme wurden gespeichert,
um sie anschließend nach R zu exportieren und dort mittels ggplot2 zu plotten.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Berechnung der Graphenkenngr\"o\ss{}en und Ergebnisse}

Nach Erstellung der Graphen wollen wir sie auf typische Kenngrößen untersuchen,
anhand dieser charakterisieren sowie paarweise vergleichen. Für die
verschiedenen Eigenschaften bietet \texttt{graph-tool} bereits implementierte
Algorithmen an, die wir verwenden wollen. Im Folgenden werden die Kenngrößen
kurz erläutert, unsere Erwartungen dargestellt und die Ergebnisse gezeigt.

\subsection{Gr\"o\ss{}e (Knotenzahl)}

Schon in der Größe der Graphen sollten sich paarweise Unterschiede erkennen
lassen. So sollten die beiden Corpora in einfacher Sprache wesentlich weniger
verschiedene Wörter enthalten. Durch die Praxis, \emph{schwierige} Wörter
durch leichter verständliche zu ersetzen, ist zu erwarten, dass erstere im einfachen
Corpus nicht vorkommen. Die Anzahl der Knoten (ohne Stopw\"orter\footnote{Es wurde
die deutsche bzw. englische Stopwortliste des NLTK benutzt. Erg\"anzt wurde
diese durch das Wort "'dass"' in neuer Rechtschreibung}) lässt sich natürlich
trivial zählen. Der \emph{deunews2010\_100K-Corpus} hat 3205 respektive 1812
Knoten, \emph{nachrichtenleicht.de} 2946 (2305), die englische Wikipedia 45897
(39212), und die \emph{simple english} Wikipedia 36582 (31045) Knoten. In der
Tat l\"asst sich also schlie\ss{}en, dass diese grundlegende Erwartung
erf\"ullt ist. Des weiteren zeigt sich, dass keine bedeutsamen Unterschiede im
Anteil der Stopw\"orter zwischen den Corpora auftreten.

\subsection{Rang der Kantengewichte}

TODO: Aussage verifizieren

Siehe Abb. \ref{fig-vdeg}

Die Kantengewichte folgen dem zipfschen Gesetz wobei sich f\"ur Leichte Sprache
feststellen l\"asst, dass die Verteilung gleichm\"a\ss{}iger ist, d.h. weniger
Spezialvokabular im "'Long Tail"' der Verteilung auftritt.

\begin{figure}
    \centering
        \includegraphics[scale=.5]{vdeg_plots.pdf}
    \caption{Verteilung der Knotengrade nach Corpus}
    \label{fig-vdeg}
\end{figure}


\subsection{Dichte}

Die Dichte eines Graphen beschreibt das Verhältnis von vorhandenen Kanten zu
potentiell möglichen Kanten in einem Graphen. Ein Wert von $1$ bedeutet, dass
jeder Knoten mit jedem anderen Verbunden ist, dem gegenüber eine $0$, dass keine
Kanten vorhanden sind. Die Dichte wird wie folgt berechnet:

$$
    \frac{|E|}{|V|(|V|-1)}
$$

$|E|$ ist die Anzahl der Kanten, $|V|$ die Anzahl der Knoten im Graphen. 

TODO: ist es hier plausibel, dass die einfachen sprachen dichter sein sollten?
es gibt ja (hoffentlich) weniger wörter, und diese wenigen sollten
dementsprechend häufiger verwendet werden und sich untereinander so leichter
verknüpfen.

Für die einfachen Sprachvarianten sollten die Graphen dichter sein als für
Standardsprache.
Wie bereits beschrieben, gibt es in ersteren weniger Vokabular, dementsprechend
weniger Knoten, die verbunden werden können. Die Vorhandenen kommen jedoch öfter 
gemeinsam vor und sind enger Verknüpft. Die Ergebnisse in Tabelle \ref{density_table}
zeigen genau dies. Der Nachrichtenleicht-Corpus ist über 31\% dichter als denews10k.
Bei den englischen Corpora beträgt der Unterschied knapp über 17\%

\begin{table}[h]
  \centering
  \begin{tabular}{ll}
    \toprule
    Corpus            & Dichte          \\
    \midrule
    nl                &  		0,0013148876 \\
    denews10k         &  		0,0009998379 \\
    wiki\_sim         &  		0,0002469035 \\
    wiki\_en          &  		0,0002109985 \\
    \bottomrule
  \end{tabular}
  \caption{\label{density_table} Die Graphdichten der Corpora}
\end{table}

\subsection{Clusterkoeffizient}

Eng mit der Dichte verwandt ist der Clusterkoeffizient. Er beschreibt die
Anzahl vorhandener Dreiecke im Graph im Verhältnis zu möglichen Dreiecken. Drei
Knoten, die jeweils paarweise verbunden sind, bilden ein Dreieck. Ziel ist ein
Messwert der Aussagt wie sehr die Knoten Cliquen bilden. Lokal betrachtet
bedeutet der Wert die Wahrscheinlichkeit die Nachbarn eines Nachbarn zu kennen.
Da wir die Graphen global miteinander vergleichen möchten verwenden wir den
globalen Clusterkoeffizienten $C$. Er berechnet sich wie folgt:

$$
    C = \frac{3\cdot\text{Anzahl der Dreiecke}}{\text{Anzahl verbundener Tripel}}
$$

Ein Tripel bezeichnet drei miteinander Verbundener Knoten, nicht
notwendigerweise ein Dreieck. Knoten A kann mit Knoten B und C verbunden sein,
während B nicht mit C verbunden ist. In \emph{Small-World-Graphen} ist dieser
Wert typischerweise hoch. 


% wir können für ausgewählte wörter vielleicht noch lokale clusterkoeffizienten
% berechnen, vielleicht für die besonders hoch verbundenen? sollten diese dann
% relativ klein sein? weil hubs ja verschiedene "teilgraphen" miteinander
% verbinden

% \paragraph{(Pseudo-)Diameter}
% \paragraph{K\"urzeste Wegl\"ange}
% die beiden fasse ich im folgendem punkt zusammen

\subsection{Minimale Wegl\"angen}
Um Small-World-Eigenschaften nachzuweisen, ist es außerdem interessant, die
Weglängen im Graphen zu untersuchen. Eine gute Kenngröße ist dabei der Durchmesser
des Graphen, also der längste minimale Weg zwischen zwei Knoten. In einer
\emph{small-world} sind alle Knoten von allen anderen in wenigen Schritten
erreichbar, typischerweise in 5 bis 7 Schritten. \texttt{graph-tool}
erlaubt zum einen die Ausgabe eines Durchmessers, wie oben beschrieben, und zum
anderen die Ausgabe eines Histogramms, welches die Vorkommen der Längen aller
kürzesten Wege zeigt.

\begin{table}[h]
  \centering
  \begin{tabular}{l*{10}{r}}
    \toprule
    Weglänge    & 1    & 2    & 3     & 4     & 5     & 6    & 7    & 8        & 9        & 10      \\
    \midrule
    nl          & 0,26 & 4,71 & 27,71 & 44,18 & 18,32 & 3,98 & 0,73 & 0,10     & $\sim$0  & $\sim$0 \\
    denews10k   & 2,00 & 3,21 & 19,19 & 44,26 & 25,31 & 6,60 & 1,11 & 0,11     & 0,01     & $\sim$0 \\
    wiki\_sim   & 0,05 & 6,01 & 47,77 & 40,20 & 5,60  & 0,36 & 0,01 & $\sim$0  & $\sim$0  & 0       \\
    wiki\_en    & 0,04 & 5,30 & 47,36 & 41,43 & 5,50  & 0,34 & 0,01 & $\sim$0  & $\sim$0  & 0       \\
  \bottomrule
  \end{tabular}
  \caption{Anteil der Weglängen paarweiser kürzester Wege zwischen allen Knoten, in Prozent}
\end{table}

TODO, folgenden abschnitt an tatsächliche ergebnisse anpassen 

Siehe Abb. \ref{fig-mdh}.

In der Tat können wir die Schrittzahl bestätigen, beispielsweise für den
nachrichtenleicht-Corpus haben wir einen Durchmesser von 6, und von
allen paarweisen Pfaden kommt dieser sehr selten vor. Bei XXX Knoten gibt
es YYY Pfade zu berechnen. Die meisten Knoten sind in drei bis vier Schritten
miteinander verbunden.
Das gleiche Bild zeigt sich bei den englischen Wikipedia-Corpora. Sowohl in 
Simple als auch in Standard English sind über 90\% der paarweisen
kürzesten Strecken vier oder weniger Schritte lang. Es gibt in beiden Fällen 
einige wenige Paare, die bis zu neun Sprünge auseinander liegen.
TODO Welche sind das? stammen die womöglich von alle von wenigen einzelnen wörtern? evtl rechtschreibfehler? 

\begin{figure}
    \centering
        \includegraphics[scale=.75]{mdh_plots.pdf}
    \caption{Verteilung der minimalen Weglängen nach Corpus}
    \label{fig-mdh}
\end{figure}

\subsection{Skalenfreiheit}
Skalenfreie Graphen zeichnen sich duch exponential verteilte Knotengrade aus.
Es gibt wenige Knoten mit sehr vielen Nachbarn, sogenannte \emph{Hubs}.
Dem gegenüber haben die anderen Knoten relativ wenige Nachbarn. 

haben wir so etwas? wir können das mit \texttt{graph-tool} über die
clustering algorithmen leicht ausrechnen!  skalieren die graphen paarweise um
ein paar knoten, oder kommen neue wichtige (hubs) wörter hinzu?

\subsection{Small-World-Eigenschaften}
Zusammengefasst zeichnen sich Small-World-Graphen durch kleine Durchmesser und
hohe Clusterkoeffizienten aus. Skalenfreiheit ist nicht zwangsläufig wichtig.
Wir konnten sowohl die kurzen Weglängen, als auch hohe Clusteringkoeffizienten
für alle Graphen nachweisen. TODO passt das? skalenfreiheit? 

\begin{table}

    \begin{tabular}{l*{4}{r}}
    \toprule
    Kenngröße & nl & denews10k & wiki\_sim & wiki\_en \\
    \midrule
    $|V|$     &       &       &       &       \\
    $|E|$     &       &       &       &       \\
    Dichte (in $10^{-4}$) & 13,148876 & 9,998379 & 2,469035 & 2,109985 \\
    Clusterkoeffizient &       &       &       &       \\
    \dots     &       &       &       &       \\
    \bottomrule
    \end{tabular}
    \caption{Übersichtstabelle Graphenkenngrößen}
\end{table}

\subsection{H\"aufigste W\"orter nach Corpus}

\subsubsection{Nachrichten-Corpora}


\begin{table}
    \begin{tabular}{l*{2}{lr}}
    \toprule
    Nr. & nl & Häufigkeit & denews10k & Häufikeit\\
    \midrule
    1  & Menschen    & 87 &  Prozent     & 48 \\
    2  & gewonnen    & 69 &  Euro        & 47 \\
    3  & Mannschaft  & 62 &  sagte       & 24 \\
    4  & Geld        & 54 &  Uhr         & 23 \\
    5  & Stadt       & 51 &  Jahren      & 23 \\
    6  & mehr        & 50 &  sei         & 18 \\
    7  & Partei      & 45 &  wurde       & 18 \\
    8  & Land        & 45 &  einfach     & 17 \\
    9  & Dortmund    & 45 &  Artikel     & 16 \\
    10 & Politiker   & 43 &  \textbf{Jahr}        & 16 \\
    11 & Spiel       & 41 &  Dollar      & 15 \\
    12 & bekommen    & 41 &  verwenden   & 15 \\
    13 & \textbf{Jahr}        & 40 &  unten       & 15 \\
    14 & viele       & 38 &  Jahre       & 15 \\
    15 & Bayern      & 36 &  stehenden   & 15 \\
    16 & Preis       & 34 &  Link        & 14 \\
    17 & München     & 34 &  möchten     & 14 \\
    18 & Verein      & 33 &  verlinken   & 14 \\
    19 & deutsche    & 33 &  kostenfrei  & 14 \\
    20 & Film        & 32 &  seit        & 14 \\
    21 & zusammen    & 32 &  dpa         & 14 \\
    22 &             &    &  Millionen   & 14 \\
    \bottomrule
    \end{tabular}
    \caption{Häufigste Worte in den Nachrichten-Corpora}
    \label{words-nachrichten}
\end{table}

Wie in Tabelle \ref{words-nachrichten} zu erkennen, gibt es nur wenige Überschneidungen des Vokabulars des nachrichtenleicht-Corpus mit denews10k.
Fett gedruckt sind diejenigen Worte, die in beiden Corpora vorkommen.
Es handelt sich hierbei nur um das Wort Jahr.
Dies deutet bereits auf eine gewisse Inhomogenität der Corpora hin.
Dieses Ergebnis ist insofern nicht überraschend, dass die Nachrichtendaten aus unterschiedlichen Quellen stammen (vgl. Abschnitt \ref{nrseiten}) und möglicherweise auch unterschiedlich normalisiert wurden.
Auch ist die jeweilige Zielgruppe der Ursprungstexte eine andere.\\
Darüber hinaus zeigt sich, dass Worte im nachrichtenleicht-Corpus generell häufiger auftreten (häufigstes Wort: Menschen, 87mal; denews: Prozent, 48mal).\\
Weiterhin scheint der nachrichtenleicht-Corpus einen Schwerpunkt auf Politik und Sport zu legen.
Erkennbar ist dies an Worten wie Stadt, Partei, Land, Politiker bzw. Mannschaft, Spiel, Bayern, München, Verein.
Im denews-Corpus treten hingegen häufiger wirtschaftsbezogene Wörter wie Prozent, Euro, Dollar oder Millionen auf.


\subsubsection{Wikipedia-Corpora}

\begin{table}
    \begin{tabular}{l*{2}{lr}}
    \toprule
    Nr. & wiki\_sim & Häufigkeit & wiki\_en & Häufikeit\\
    \midrule
     1 & \textbf{born}       & 71 & \textbf{born}         & 86\\
     2 & \textbf{American}   & 52 & \textbf{American}     & 58\\
     3 & \textbf{commune}    & 49 & \textbf{commune}      & 52\\
     4 & \textbf{France}     & 47 & \textbf{France}       & 50\\
     5 & \textbf{won}        & 41 & \textbf{department}   & 49\\
     6 & \textbf{department} & 41 & film                  & 46\\
     7 & movie               & 36 & \textbf{television}   & 43\\
     8 & \textbf{television} & 35 & \textbf{won}          & 42\\
     9 & died                & 33 & \textbf{League}       & 40\\
    10 & people              & 33 & \textbf{County}       & 40\\
    11 & NHL                 & 31 & \textbf{Award}        & 39\\
    12 & actor               & 31 & University            & 39\\
    13 & \textbf{World}      & 30 & series                & 38\\
    14 & United              & 29 & \textbf{released}     & 38\\
    15 & \textbf{League}     & 28 & album                 & 38\\
    16 & National            & 28 & professional          & 35\\
    17 & \textbf{years}      & 28 & located               & 34\\
    18 & \textbf{released}   & 28 & season                & 33\\
    19 & \textbf{Award}      & 27 & \textbf{World}        & 33\\
    20 & \textbf{County}     & 27 & \textbf{years}        & 31\\
    21 & played              & 27 & city                  & 31\\
    22 & year                & 27 &                       & \\
    \bottomrule
    \end{tabular}
    \caption{Häufigste Worte in den Wiki-Corpora}
    \label{words-wiki}
\end{table}

Der Berechnung der häufigsten Worte der Wiki-Corpora (vgl. Tabelle \ref{words-wiki}) wurde ein Kookkurrenz-Threshold von $t=0.005$ zugrunde gelegt.
Dies entspricht einem Kookkurrenzwert von $\geq 200$.\\
Die Ergebnisse zeigen eine hohe Übereinstimmung des Vokabulars zwischen beiden Wikipedia-Versionen, was für eine größere thematische und strukturelle Homogenität als bei den Nachrichtencorpora spricht.
Die häufigsten vier Worte sind bei beiden Corpora identisch.
Insgesamt stimmen über $50\%$ der Worte überein - lediglich die Rangfolge variiert leicht.
Interessant ist, dass die Simple-English-Variante das Wort \textit{movie} favorisiert, während in der Standardsprache das Synonym \textit{film} verwendet wird.

Anders als bei den Nachrichten-Corpora sind hier bei der Leichte-Sprache-Variante geringere Worthäufigkeiten als bei der Standardsprache zu beobachten.
Die Größenordnung dieses Unterschieds ist allerdings nicht so gravierend wie bei den ersteren.\\
Eine mögliche Erklärung könnte sein, dass wir (wie in Abschnitt \ref{datextr} beschrieben) beim Erstellen des Corpus die Artikel nach Satzanzahl, nicht nach Wortanzahl normalisiert haben.
Da die Standard-English-Wikipedia längere Sätze verwendet als die Einfache, sind in logischer Folge auch größere Worthäufigkeiten zu erwarten, denn längere Sätze bedeuten mehr Wörter.



\subsection{Kookkurrenzgraphen häufig vorkommender Wörter}
%Zur Erzeugung der Grafiken wurde der Threshold auf 100 erhöht, damit unwichtige Kanten die Grafiken nicht überladen.
%TODO: Threshold je nach Corpus verschieden

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{thebibliography}{9}
    \bibitem{Biemann2004} C. Biemann, S. Bordag, G. Heyer, U. Quasthoff, and C. Wolff, “Language-Independent Methods for Compiling Monolingual Lexical Data,” in Computational Linguistics and Intelligent Text Processing, vol. 2945, A. Gelbukh, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2004, pp. 217–228.
	\bibitem{Hu2005} Y. Hu, “Efficient, High-Quality Force-Directed Graph Drawing,” The Mathematica Journal, vol. 10, no. 1, pp. 37–71, 2006.
    \bibitem{Newman2003} M. E. J. Newman, “The Structure and Function of Complex Networks,” SIAM Review, vol. 45, no. 2, pp. 167–256, 2003.
\end{thebibliography}

\listoftables

\listoffigures

\end{document}
