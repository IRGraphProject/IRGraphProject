\documentclass[11pt, a4paper]{article}
\usepackage{ifxetex}
\usepackage{amsmath}
\usepackage[hyperindex,colorlinks,citecolor=cyan,linkcolor=gray,urlcolor=blue
			]{hyperref}

\ifxetex
	\usepackage{fontspec}
	\usepackage{unicode-math}
	\setmainfont[Ligatures=TeX,
		Extension=.otf,
		BoldFont=*-bold,
		UprightFont=*-regular,
		ItalicFont=*-italic,
		BoldItalicFont=*-bolditalic,
	SmallCapsFeatures={Letters=SmallCaps}]{texgyrepagella}
	\setmathfont[Ligatures=TeX]{texgyrepagella-math.otf}
	
	% set up Heros (Helvetica)
	\setsansfont[Ligatures=TeX,
		Extension=.otf,
		BoldFont=*-bold,
		UprightFont=*-regular,
		ItalicFont=*-italic,
		BoldItalicFont=*-bolditalic,
	SmallCapsFeatures={Letters=SmallCaps}]{texgyreheros}
\else
	\usepackage[utf8]{inputenc}
	\usepackage[T1]{fontenc}
\fi

\usepackage{ngerman}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{footnote}
\makesavenoteenv{tabular} % I want footnotes in my tables
\makesavenoteenv{table}

\usepackage{svg}
\usepackage{graphicx}
\graphicspath{{../img/}}

\title{Grapheigenschaften auf Kookkurrenzgraphen in Leichter und Standardsprache auf Wikipedia- und Nachrichtencorpora}
\author{Author 1, Author 2, Author 3\\Modul "`Fortgeschrittene Methoden des Information Retrieval"'}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation und Ziel}

Leichte oder auch Einfache Sprache ist eine Untermenge der deutschen Sprache,
die auf besonders leichte Verst\"andlichkeit optimiert ist. Sie umfasst unter
anderem spezielle Sprachregelungen, typographische Empfehlungen und
Rechtschreibregeln. 

Das \emph{Netzwerk Leichte Sprache} definiert folgende grundlegenden
Eigenschaften Leichter Sprache:

% TODO: Quellenangabe für das Folgende!
\begin{enumerate}
	\item Benutzen Sie einfache W\"orter
	\item Benutzen Sie W\"orter, die etwas genau beschreiben
	\item Benutzen Sie bekannte W\"orter und verzichten sie auf Fachw\"orter und Fremdw\"orter
	\item Benutzen Sie immer die gleichen W\"orter f\"ur gleiche Dinge
	\item Benutzen Sie kurze W\"orter
	\item Verzichten Sie auf Abk\"urzungen
	\item Benutzen Sie Verben
	\item Vermeiden Sie den Genitiv und Konjunktiv
	\item Vermeiden Sie Kolloquialismen und bildliche Sprache
	\item Benutzen Sie Ziffern anstatt von Worten
	\item Schreiben Sie kurze S\"atze, die nur eine Aussage enthalten
	\item Benutzen Sie einen einfachen Satzbau
	\item Schreiben Sie jeden Satz in eine Zeile
\end{enumerate}

Das englische \"Aquivalent zu Leichter Sprache ist \emph{Simple English}. Es
existieren verschiedene Modelle des Simple English, welche unterschiedliche
Ziele erreichen sollen -- z.B. das \emph{Simplified Technical English}, eine
kontrollierte Sprache f\"ur technische Handb\"ucher. Aufgrund dieser
konkurrierenden Ans\"atze gibt es keine einheitliche Definition oder
Sprachpraxis des Simple English. %So sind z.B. i TODO ???


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Werkzeuge}

\subsection{Programmiersprache Python}
Nach sprachunanbängigen Recherchen über verwendbare Bibliotheken haben wir uns
wegen der bereits implementierten Graphalgorithmen in \texttt{graph-tool} für
die Programmiersprache Python\footnote{\url{https://www.python.org/}}
entschieden.
Wegen der nativen Unicode-Unterstützung wurde Python 3 gewählt.

\subsection{Graphen-Bibliothek \texttt{graph-tool}}
\texttt{graph-tool}\footnote{\url{http://graph-tool.skewed.de/}} ist ein
Python-Modul, welches der Erstellung, Manipulation und statistischen
Auswertung von Graphen dient. Es stellt im Kern ein C++-Wrapper um die Boost
Graph Library dar, wodurch eine \"ahnliche Performanz zu nativen C-Bibliotheken
erreicht wird. Es ist zus\"atzlich in der Lage, Graphen mit modernen Techniken
zu visualisieren und \"ubliche Ma\ss{}e wie Clustering-Koeffizienten, Knoten-
und Kantengrade und Durchmesser zu berechnen.

\subsection{NLP-Framework NLTK}
Das Natural Language Toolkit (NLTK\footnote{\url{http://www.nltk.org/}}) ist
ein Framework f\"ur die Verarbeitung natürlicher Sprache in Python.
Die verwendeten Stopwortlisten für Deutsch und Englisch stammen aus der
Bibliothek nltk-data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datenbasis}

Als Datenbasis wurden Quellen gew\"ahlt, die sich durch eine gro\ss{}e
semantische und logische N\"ahe auszeichnen, namentlich Nachrichtenseiten in
deutscher und Wikipediaartikel in englischer Sprache.

\subsection{Nachrichtenseiten}
\label{nrseiten}

\subsubsection{nachrichtenleicht.de (nl)}

Nachrichtenleicht ist ein Dienst des Deutschlandfunks, welcher einmal
w\"ochentlich die wichtigsten Nachrichten der vorangegangenen Woche in Leichter
Sprache zusammenfasst. Er soll die mehreren Millionen Menschen in Deutschland
erreichen, die aus verschiedenen Gr\"unden von konventionellen
Nachrichtenangeboten ausgeschlossen sind.

\subsubsection{deunews2010\_10K Corpus der ASV (denews10k)}

Als korrespondierende Datenquelle in Standardsprache wurde der
deunews\-2010\_10K-Corpus des Lehrstuhls f\"ur automatische Sprachverarbeitung
(ASV) der Universit\"at Leipzig gew\"ahlt. Dieser weist eine \"ahnliche Satzzahl
zu den von nachrichtenleicht extrahierten Text aus und wurde bereits von der ASV
normalisiert \cite{Quasthoff2006}.

\subsection{Wikipedia (wiki\_sim bzw. wiki\_en)}
\label{corp-wiki}
Die zweite Datenquelle sind Artikel aus der Wikipedia in \emph{Simple} und
\emph{Standard English}. Mit fast 120.000 Lemmata stellt die Simple English
Wikipedia eine der gr\"o\ss{}ten \"offentlich frei verf\"ugbaren Textsammlungen
in einer einfachen Sprachvariante dar.
Zus\"atzlich ist ein Dokumentenalignment zwischen den zwei Sprachvarianten
vorhanden, was die einfache Erstellung zweier inhaltlich homogener Corpora
gestattet.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vorverarbeitung}

\subsection{Datenextraktion und Erstellung der Corpora}
\label{datextr}

Mittels eines selbstgeschriebenen Crawlers wurden die relevanten Textmengen
heruntergeladen, extrahiert und zu Corpora zusammengefasst.

Nachrichtenleicht wurde rekursiv von der Startseite aus gequeriet
und die resultierenden Texte in einer MongoDB gespeichert.
Dabei wurden Überschriften, Teaser und Nachrichtentext jeweils seperat erfasst
und schließlich Teaser und Nachrichteninhalt zum Corpus hinzugefügt.

Zur Erstellung der Wikipedia-Corpora wurde zunächst eine Liste aller
in Simple English verfügbaren Artikel erstellt. Anschließend wurden diese und
die jeweils korrespondierenden Artikel in Standard English heruntergeladen und
im Rohformat (mit eingebettetem MediaWiki-Markup) in einer JSON-Datei gespeichert.

Die Entfernung des Markups stellte sich dabei als eine nichttriviale Aufgabe dar.
Ein vollständiger Parser\footnote{MediaWiki
Parser from Hell (\url{http://mwparserfromhell.rtfd.org})} erwies sich für die
entsprechende Datenmenge (ca. 2~GB Rohdaten) als deutlich zu langsam
und somit nicht verwendbar.
\\
Mit einem Python-Skript, das Markup mit Hilfe regul\"arer Ausdr\"ucke entfernt
bzw. umwandelt, konnten jedoch zufriedenstellende Ergebnisse mit vertretbarem
Zeitaufwand erzielt werden.

Nach Entfernen des Wiki-Markups wurden die Artikel auf die Satzanzahl
des jweils k\"urzeren Artikels reduziert und anschliessend in einer MongoDB
gespeichert.


\subsection{Berechnung der Kookkurrenzen}

Die Kookkurrenzen wurden von mittels der Pipeline des Lehrsstuhls f\"ur
Automatische Sprachverarbeitung für uns berechnet und in eine MySQL-Datenbank
gespeichert.


\subsection{Erstellung der Graphen}

Die errechneten Kookkurrenz-Paare wurden zunächst gefiltert. Dabei wurden 
\begin{itemize}
    \item Lemmata, welche Leerzeichen, Kommata oder andere Satzzeichen enthalten
    \item Lemmata der Länge 1
    \item Kookkurrenzen zwischen zwei identischen Lemmata
\end{itemize}
entfernt.
Außerdem wurden -- da wir mit einem ungerichteten Graphen arbeiten -- identische
Kookkurrenzpaare entfernt, so dass am Ende jedes Paar nur einmal auftrat.
Die verbleibenden Kookkurrenzpaare wurden in \texttt{graph-tool}-Objekte geparst
und zu einem corpusumfassenden Kokkurrenzgraphen zusammengesetzt.
Subgraphen aus den Corpora wurden mittels eines \texttt{force-directed graph}
nach \cite{Hu2006} gelayoutet.

Um sinnvolle Berechnungen von Weglängen etc. anzustellen, erwies es sich als
notwendig, den so erzeugten Graphen auf die größte Zusammenhangskomponente zu
reduzieren und nicht verbundene Subgraphen zu verwerfen.
Die Kanten- und Knotenanzahl vor und nach diesem Schritt sind Tabelle
\ref{tab-zsf} zu entnehmen.

Anschließend wurden die relevanten Berechnungen (vgl. Abschnitt
\ref{berechnung-ergebnisse}) durchgeführt und Ergebnisse ausgegeben.
Auch die Daten zur Erstellung der Histogramme wurden in diesem Schritt gespeichert,
um sie anschließend nach R zu importieren und dort mittels ggplot2 zu plotten.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Berechnung der Graphenkenngr\"o\ss{}en und Ergebnisse}
\label{berechnung-ergebnisse}

Nach Erstellung der Graphen sollen diese auf typische Kenngrößen untersucht,
anhand dieser charakterisiert sowie paarweise verglichen werden. Für
verschiedene Eigenschaften bietet \texttt{graph-tool} bereits implementierte
Algorithmen an. Im Folgenden werden die Kenngrößen kurz erläutert, unsere
Erwartungen dargestellt und die Ergebnisse gezeigt.

\subsection{Gr\"o\ss{}e (Knotenzahl)}

Schon in der Größe der Graphen sollten sich paarweise Unterschiede erkennen
lassen. So sollten die beiden Corpora in einfacher Sprache wesentlich weniger
verschiedene Wörter enthalten. Durch die Praxis, \emph{schwierige} Wörter
durch leichter verständliche zu ersetzen, ist zu erwarten, dass erstere im einfachen
Corpus nicht vorkommen. Die Anzahl der Knoten lässt sich trivial auszählen.
% TODO:
% -Folgende Zahlen überprüfen!
% -Wir verwenden nur die größte Zusammenhangskomponente. (Das hat mit
%  Stopwörtern nichts zu tun!)
Der \emph{deunews2010\_10K-Corpus} hat 3205 respektive 1812
Knoten, \emph{nachrichtenleicht.de} 2946 (2305), die englische Wikipedia 45897
(39212), und die \emph{simple english} Wikipedia 36582 (31045) Knoten. In der
Tat l\"asst sich also schlie\ss{}en, dass diese grundlegende Erwartung
erf\"ullt ist.

\subsection{Rang der Kantengewichte}

Siehe Abb. \ref{fig-vdeg}.

%TODO: Folgende Aussage verifizieren

Die Kantengewichte folgen dem Zipfschen Gesetz, wobei sich f\"ur Leichte Sprache
feststellen l\"asst, dass die Verteilung gleichm\"a\ss{}iger ist, d.h. weniger
Spezialvokabular im "'Long Tail"' der Verteilung auftritt.

\begin{figure}
    \centering
        \includegraphics[scale=.5]{vdeg_plots.pdf}
    \caption{Verteilung der Knotengrade nach Corpus}
    \label{fig-vdeg}
\end{figure}


\subsection{Dichte}

Die Dichte eines Graphen beschreibt das Verhältnis von vorhandenen Kanten zu
potentiell möglichen Kanten in einem Graphen. Ein Wert von $1$ bedeutet, dass
jeder Knoten mit jedem anderen Verbunden ist, dem gegenüber eine $0$, dass keine
Kanten vorhanden sind. Die Dichte wird wie folgt berechnet:

$$
    \frac{|E|}{|V|(|V|-1)}
$$

$|E|$ ist die Anzahl der Kanten, $|V|$ die Anzahl der Knoten im Graphen. 

%TODO: ist es hier plausibel, dass die einfachen sprachen dichter sein sollten?
%es gibt ja (hoffentlich) weniger wörter, und diese wenigen sollten
%dementsprechend häufiger verwendet werden und sich untereinander so leichter
%verknüpfen.

Für die einfachen Sprachvarianten sollten die Graphen dichter sein als für
Standardsprache.
Wie bereits beschrieben, gibt es in ersteren weniger Vokabular, dementsprechend
weniger Knoten, die verbunden werden können. Die Vorhandenen kommen jedoch öfter 
gemeinsam vor und sind enger Verknüpft. Die Ergebnisse in Tabelle \ref{density_table}
zeigen genau dies. Der Nachrichtenleicht-Corpus ist über 31\% dichter als denews10k.
Bei den englischen Corpora beträgt der Unterschied knapp über 17\%
% TODO: Obige Zahlen an die tatsächlichen Ergebnisse anpassen

\begin{table}[h]
  \centering
  \begin{tabular}{ll}
    \toprule
    Corpus            &         Dichte       \\
    \midrule
    nl                &  		0,0015090239 \\
    denews10k         &  		0,0007737342 \\
    wiki\_sim         &  		0,0002844456 \\
    wiki\_en          &  		0,0002489951 \\
    \bottomrule
  \end{tabular}
  \caption{\label{density_table} Graphdichte nach Corpus}
\end{table}

\subsection{Clusterkoeffizient}

Eng mit der Dichte verwandt ist der Clusterkoeffizient. Er beschreibt die
Anzahl vorhandener Dreiecke im Graph im Verhältnis zu möglichen Dreiecken. Drei
Knoten, die jeweils paarweise verbunden sind, bilden ein Dreieck. Ziel ist ein
Messwert, der aussagt, wie sehr die Knoten Cliquen bilden. Lokal betrachtet
bedeutet der Wert die Wahrscheinlichkeit, die Nachbarn eines Nachbarn zu kennen.
Da wir die Graphen global miteinander vergleichen möchten, verwenden wir den
globalen Clusterkoeffizienten $C$. Er berechnet sich wie folgt:

$$
    C = \frac{3\cdot\text{Anzahl der Dreiecke}}{\text{Anzahl verbundener Tripel}}
$$

Ein Tripel bezeichnet drei miteinander Verbundener Knoten, nicht
notwendigerweise ein Dreieck. Knoten A kann mit Knoten B und C verbunden sein,
während B nicht mit C verbunden ist. In \emph{Small-World-Graphen} ist dieser
Wert typischerweise hoch. 

% TODO:
% wir können für ausgewählte wörter vielleicht noch lokale clusterkoeffizienten
% berechnen, vielleicht für die besonders hoch verbundenen? sollten diese dann
% relativ klein sein? weil hubs ja verschiedene "teilgraphen" miteinander
% verbinden

% \paragraph{(Pseudo-)Diameter}
% \paragraph{K\"urzeste Wegl\"ange}
% die beiden fasse ich im folgendem punkt zusammen

\subsection{Durchmesser und minimale Wegl\"angen}
Um Small-World-Eigenschaften nachzuweisen, ist es außerdem interessant, die
Weglängen im Graphen zu untersuchen. Eine gute Kenngröße ist dabei der Durchmesser
des Graphen, also der längste minimale Weg zwischen zwei Knoten. In einer
\emph{small-world} sind alle Knoten von allen anderen in wenigen Schritten
erreichbar, typischerweise in 5 bis 7 Schritten. \texttt{graph-tool}
erlaubt zum einen die Ausgabe eines Durchmessers, wie oben beschrieben, und zum
anderen die Ausgabe eines Histogramms, welches die Vorkommen der Längen aller
kürzesten Wege zeigt (siehe Abb. \ref{fig-mdh}).

\begin{table}[h]
  \centering
  \begin{tabular}{l*{10}{r}}
    \toprule
    Weglänge    & 1    & 2    & 3     & 4     & 5     & 6    & 7    & 8        & 9        & 10      \\
    \midrule
    nl          & 0,26 & 4,71 & 27,71 & 44,18 & 18,32 & 3,98 & 0,73 & 0,10     & $\sim$0  & $\sim$0 \\
    denews10k   & 2,00 & 3,21 & 19,19 & 44,26 & 25,31 & 6,60 & 1,11 & 0,11     & 0,01     & $\sim$0 \\
    wiki\_sim   & 0,05 & 6,01 & 47,77 & 40,20 & 5,60  & 0,36 & 0,01 & $\sim$0  & $\sim$0  & 0       \\
    wiki\_en    & 0,04 & 5,30 & 47,36 & 41,43 & 5,50  & 0,34 & 0,01 & $\sim$0  & $\sim$0  & 0       \\
  \bottomrule
  \end{tabular}
  \caption{Anteil der Weglängen paarweiser kürzester Wege zwischen allen Knoten, in Prozent}
\end{table}

% TODO: folgenden abschnitt an tatsächliche ergebnisse anpassen 
In der Tat können wir die Schrittzahl bestätigen, beispielsweise für den
nachrichtenleicht-Corpus haben wir einen Durchmesser von 6, und von
allen paarweisen Pfaden kommt dieser sehr selten vor. Bei XXX Knoten gibt
es YYY Pfade zu berechnen. Die meisten Knoten sind in drei bis vier Schritten
miteinander verbunden.
Das gleiche Bild zeigt sich bei den englischen Wikipedia-Corpora. Sowohl in 
Simple als auch in Standard English sind über 90\% der paarweisen
kürzesten Strecken vier oder weniger Schritte lang. Es gibt in beiden Fällen 
einige wenige Paare, die bis zu neun Sprünge auseinander liegen.
% TODO: Welche sind das? stammen die womöglich von alle von wenigen einzelnen wörtern? evtl rechtschreibfehler? 

\begin{figure}
    \centering
        \includegraphics[scale=.75]{mdh_plots.pdf}
    \caption{Verteilung der minimalen Weglängen nach Corpus}
    \label{fig-mdh}
\end{figure}

\subsection{Skalenfreiheit}
Skalenfreie Graphen zeichnen sich duch exponential verteilte Knotengrade aus.
Es gibt wenige Knoten mit sehr vielen Nachbarn, sogenannte \emph{Hubs}.
Dem gegenüber haben die anderen Knoten relativ wenige Nachbarn. 

% TODO:
%haben wir so etwas? wir können das mit \texttt{graph-tool} über die
%clustering algorithmen leicht ausrechnen!  skalieren die graphen paarweise um
%ein paar knoten, oder kommen neue wichtige (hubs) wörter hinzu?

\subsection{Small-World-Eigenschaften}
Zusammengefasst zeichnen sich Small-World-Graphen durch kleine Durchmesser und
hohe Clusterkoeffizienten aus. Skalenfreiheit ist nicht zwangsläufig wichtig.
Wir konnten sowohl die kurzen Weglängen, als auch hohe Clusteringkoeffizienten
für alle Graphen nachweisen.
%TODO passt das? skalenfreiheit? 

\subsection{Zusammenfassung der Berechnungen}
\begin{table}
    \begin{tabular}{l*{4}{r}}
    \toprule
    Kenngröße & nl & denews10k & wiki\_sim & wiki\_en \\
    \midrule
    $|V|$                       & 2946      & 3205      & 36582     & 45897  \\
    $|E|$                       & 11639     & 7669      & 371559    & 514489 \\
    $|V|$\footnotemark[6]       & 2766      & 3142      & 36131     & 45446  \\
    $|E|$\footnotemark[6]       & 11541     & 7636      & 371319    & 514248 \\
    Dichte (in $10^{-4}$)       & 15,090239 & 7,737342  & 2,844456  & 2,489951 \\
    Clusterkoeff. max ($10^{-2}$) & 6,451559 & 3,390617 & 4,040342  & 4,229930 \\
    Clusterkoeff. min ($10^{-2}$) & 0,626338 & 0,470389 & 0,612476  & 0,481688 \\
    \dots                       &           &           &           &          \\
    \bottomrule
    \end{tabular}
    \caption{Übersichtstabelle Graphenkenngrößen}
    \label{tab-zsf}
\end{table}
% TODO: nach dem Endlayout sicherstellen, dass die folgene Fußnote an der richtigen
% Stelle auftaucht!
\footnotetext[6]{größte Zusammenhangskomponente}

In Tabelle \ref{tab-zsf} befindet sich eine Zusammenfassung der für die
jeweiligen Corpora berechneten Kenngrößen.


\subsection{H\"aufige W\"orter}

\subsubsection{Nachrichten-Corpora}

\begin{table}
    \begin{tabular}{l*{2}{lr}}
    \toprule
    Nr. & nl & Häufigkeit & denews10k & Häufikeit\\
    \midrule
    1  & Menschen    & 87 &  Prozent     & 48 \\
    2  & gewonnen    & 69 &  Euro        & 47 \\
    3  & Mannschaft  & 62 &  sagte       & 24 \\
    4  & Geld        & 54 &  Uhr         & 23 \\
    5  & Stadt       & 51 &  Jahren      & 23 \\
    6  & mehr        & 50 &  sei         & 18 \\
    7  & Partei      & 45 &  wurde       & 18 \\
    8  & Land        & 45 &  einfach     & 17 \\
    9  & Dortmund    & 45 &  Artikel     & 16 \\
    10 & Politiker   & 43 &  \textbf{Jahr}        & 16 \\
    11 & Spiel       & 41 &  Dollar      & 15 \\
    12 & bekommen    & 41 &  verwenden   & 15 \\
    13 & \textbf{Jahr}        & 40 &  unten       & 15 \\
    14 & viele       & 38 &  Jahre       & 15 \\
    15 & Bayern      & 36 &  stehenden   & 15 \\
    16 & Preis       & 34 &  Link        & 14 \\
    17 & München     & 34 &  möchten     & 14 \\
    18 & Verein      & 33 &  verlinken   & 14 \\
    19 & deutsche    & 33 &  kostenfrei  & 14 \\
    20 & Film        & 32 &  seit        & 14 \\
    21 & zusammen    & 32 &  dpa         & 14 \\
    22 &             &    &  Millionen   & 14 \\
    \bottomrule
    \end{tabular}
    \caption{Häufigste Worte in den Nachrichten-Corpora}
    \label{words-nachrichten}
\end{table}

Wie in Tabelle \ref{words-nachrichten} zu erkennen, gibt es nur wenige
Überschneidungen des Vokabulars des nachrichtenleicht-Corpus mit denews10k.
Fett gedruckt sind in den Tabellen diejenigen Worte, die in beiden Corpora
vorkommen.
Bei den Nachrichten-Corpora handelt es sich nur um das Wort Jahr.
Dies deutet bereits auf eine gewisse thematische Inhomogenität der Corpora hin.
Dieses Ergebnis ist insofern nicht überraschend, dass die Nachrichtendaten aus
unterschiedlichen Quellen stammen (vgl. Abschnitt \ref{nrseiten}) und
möglicherweise auch unterschiedlich normalisiert wurden.
Auch ist die jeweilige Zielgruppe der Ursprungstexte eine andere.

Darüber hinaus zeigt sich, dass Worte im nachrichtenleicht-Corpus generell
häufiger auftreten (häufigstes Wort: Menschen, 87mal; denews: Prozent, 48mal).
Dies bestätigt die Vermutung aus Abschnitt \ref{???}\\
Weiterhin wird ersichtlich, dass der nachrichtenleicht-Corpus einen Schwerpunkt
auf Politik und Sport legt.
Erkennbar ist dies am häufigen Auftreten der Worte wie Stadt, Partei, Land,
Politiker bzw. Mannschaft, Spiel, Bayern, München, Verein.
Im denews-Corpus treten hingegen häufiger wirtschaftsbezogene Wörter wie Prozent,
Euro, Dollar oder Millionen auf.


\subsubsection{Wikipedia-Corpora}

\begin{table}
    \begin{tabular}{l*{2}{lr}}
    \toprule
    Nr. & wiki\_sim & Häufigkeit & wiki\_en & Häufikeit\\
    \midrule
     1 & \textbf{born}       & 71 & \textbf{born}         & 86\\
     2 & \textbf{American}   & 52 & \textbf{American}     & 58\\
     3 & \textbf{commune}    & 49 & \textbf{commune}      & 52\\
     4 & \textbf{France}     & 47 & \textbf{France}       & 50\\
     5 & \textbf{won}        & 41 & \textbf{department}   & 49\\
     6 & \textbf{department} & 41 & film                  & 46\\
     7 & movie               & 36 & \textbf{television}   & 43\\
     8 & \textbf{television} & 35 & \textbf{won}          & 42\\
     9 & died                & 33 & \textbf{League}       & 40\\
    10 & people              & 33 & \textbf{County}       & 40\\
    11 & NHL                 & 31 & \textbf{Award}        & 39\\
    12 & actor               & 31 & University            & 39\\
    13 & \textbf{World}      & 30 & series                & 38\\
    14 & United              & 29 & \textbf{released}     & 38\\
    15 & \textbf{League}     & 28 & album                 & 38\\
    16 & National            & 28 & professional          & 35\\
    17 & \textbf{years}      & 28 & located               & 34\\
    18 & \textbf{released}   & 28 & season                & 33\\
    19 & \textbf{Award}      & 27 & \textbf{World}        & 33\\
    20 & \textbf{County}     & 27 & \textbf{years}        & 31\\
    21 & played              & 27 & city                  & 31\\
    22 & year                & 27 &                       & \\
    \bottomrule
    \end{tabular}
    \caption{Häufigste Worte in den Wiki-Corpora}
    \label{words-wiki}
\end{table}

Der Berechnung der häufigsten Worte der Wiki-Corpora (vgl. Tabelle
\ref{words-wiki}) wurde ein Kookkurrenz-Threshold von $t=0.005$ zugrunde gelegt.
Dies entspricht einem Kookkurrenzwert von $\geq 200$.\\
Die Ergebnisse zeigen eine hohe Übereinstimmung des Vokabulars zwischen beiden
Wikipedia-Versionen, was für eine größere thematische und strukturelle
Homogenität als bei den Nachrichtencorpora spricht.
Die häufigsten vier Worte sind bei beiden Corpora identisch.
Insgesamt stimmen über $50\%$ der Worte überein - lediglich die Rangfolge
variiert leicht.
Dies ist angesichts des Dokumentenalignments zwischen beiden Corpora (vgl.
Abschnitt \ref{corp-wiki}) zu erwarten gewesen.

Interessant ist, dass die Simple-English-Variante etwa das Wort \textit{movie}
favorisiert, während in der Standardsprache das Synonym \textit{film} verwendet
wird.

Anders als bei den Nachrichten-Corpora sind hier bei der Leichte-Sprache-Variante
geringere Worthäufigkeiten als bei der Standardsprache zu beobachten.
Die Größenordnung dieses Unterschieds ist allerdings nicht so gravierend wie bei
den ersteren.\\
Eine mögliche Erklärung könnte sein, dass wir (wie in Abschnitt \ref{datextr}
beschrieben) beim Erstellen des Corpus die Artikel nach Satzanzahl, nicht nach
Wortanzahl normalisiert haben.
Da die Standard-English-Wikipedia längere Sätze verwendet als die Einfache,
sind in logischer Folge auch größere Worthäufigkeiten zu erwarten, denn längere
Sätze bedeuten mehr Wörter.



\section{Kookkurrenzgraphen häufiger Wörter}
%Zur Erzeugung der Grafiken wurde der Threshold auf XYZ erhöht, damit unwichtige Kanten die Grafiken nicht überladen.
%TODO: Threshold je nach Corpus verschieden

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{thebibliography}{9}
% TODO:
% - remove unused citations/bibitems
% - sort remaining ones in order of appearance in the text
    \bibitem{Newman2003} M. E. J. Newman, “The Structure and Function of Complex Networks,” \emph{SIAM Review}, vol. 45, no. 2, pp. 167–256, 2003.
    \bibitem{Biemann2004} C. Biemann, S. Bordag, G. Heyer, U. Quasthoff, and C. Wolff, “Language-Independent Methods for Compiling Monolingual Lexical Data,” in \emph{Computational Linguistics and Intelligent Text Processing}, vol. 2945, A. Gelbukh, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2004, pp. 217–228.
    \bibitem{Hu2006} Y. Hu, “Efficient, High-Quality Force-Directed Graph Drawing,” \emph{The Mathematica Journal}, vol. 10, no. 1, pp. 37–71, 2006.
    \bibitem{Quasthoff2006}U. Quasthoff, M. Richter, and C. Biemann, “Corpus Portal for Search in Monolingual Corpora,” in \emph{Proceedings of LREC-06}, Genoa, Italy, 2006, pp. 1799–1802.
    \bibitem{Telesford2011}Q. K. Telesford, K. E. Joyce, S. Hayasaka, J. H. Burdette, and P. J. Laurienti, “The Ubiquity of Small-World Networks,” \emph{Brain Connectivity}, vol. 1, no. 5, pp. 367–375, 2011.
\end{thebibliography}

\listoftables

\listoffigures

\end{document}
